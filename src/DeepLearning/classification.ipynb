{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Internal imports\n",
    "from src.utils import memoize\n",
    "from src.deepLearning import get_similar_nodes\n",
    "from src.deepLearning import node2vec_embedding\n",
    "\n",
    "from src.NetworkGraphs import NetworkGraphs\n",
    "\n",
    "from src.machineLearning import get_communities\n",
    "from src.visualisation_src.ML_visualisation import generate_static_cluster\n",
    "from src.visualisation_src.DL_visualisation import TSNE_visualisation\n",
    "from torch_geometric.utils import train_test_split_edges"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T11:06:32.796107Z",
     "end_time": "2023-04-11T11:06:32.801810Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, embed_dim):\n",
    "        super(GAE, self).__init__()\n",
    "        self.encoder = GCNConv(num_features, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        z = self.encoder(x, edge_index)\n",
    "        x = self.decoder(z)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.664413Z",
     "end_time": "2023-04-11T10:40:27.682637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, embed_dim):\n",
    "        super(GAE2, self).__init__()\n",
    "        self.encoder = SAGEConv(num_features, hidden_dim)\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        z = self.encoder(x, edge_index)\n",
    "        x = self.decoder(z)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.667261Z",
     "end_time": "2023-04-11T10:40:27.683010Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unsupervised_loss(recon_x, x):\n",
    "    mse_loss = F.mse_loss(recon_x, x)\n",
    "    return mse_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.684392Z",
     "end_time": "2023-04-11T10:40:27.689378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def contrastive_loss(embeddings, positive_pairs, negative_pairs, margin=1.0):\n",
    "\n",
    "    positive_distances = torch.norm(embeddings[positive_pairs[:, 0]] - embeddings[positive_pairs[:, 1]], dim=1)\n",
    "    negative_distances = torch.norm(embeddings[negative_pairs[:, 0]] - embeddings[negative_pairs[:, 1]], dim=1)\n",
    "\n",
    "    positive_loss = torch.mean(torch.square(positive_distances))\n",
    "    negative_loss = torch.mean(torch.square(torch.clamp(margin - negative_distances, min=0.0)))\n",
    "\n",
    "    loss = 0.5 * (positive_loss + negative_loss)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.690843Z",
     "end_time": "2023-04-11T10:40:27.693354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def pairs_to_indices(data, positive_pairs, negative_pairs):\n",
    "    node_to_index = {node: i for i, node in enumerate(data.mapping)}\n",
    "\n",
    "    positive_indices = np.array([[node_to_index.get(u), node_to_index.get(v)] for u, v in positive_pairs if u in node_to_index and v in node_to_index])\n",
    "    negative_indices = np.array([[node_to_index.get(u), node_to_index.get(v)] for u, v in negative_pairs if u in node_to_index and v in node_to_index])\n",
    "\n",
    "    return positive_indices, negative_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.695487Z",
     "end_time": "2023-04-11T10:40:27.697857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_pairs(networkx_graph, num_negative_pairs=None):\n",
    "    nodes = list(networkx_graph.nodes())\n",
    "\n",
    "    # Positive pairs\n",
    "    positive_pairs = np.array([[u, v] for u, v in networkx_graph.edges])\n",
    "\n",
    "    # Negative pairs\n",
    "    if num_negative_pairs is None:\n",
    "        num_negative_pairs = len(positive_pairs)\n",
    "\n",
    "    negative_pairs = []\n",
    "    while len(negative_pairs) < num_negative_pairs:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not networkx_graph.has_edge(u, v):\n",
    "            negative_pairs.append([u, v])\n",
    "    negative_pairs = np.array(negative_pairs)\n",
    "\n",
    "    return positive_pairs, negative_pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.699327Z",
     "end_time": "2023-04-11T10:40:27.701592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, device):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.to(device))\n",
    "    loss = unsupervised_loss(out[data.train_mask], data.x[data.train_mask])\n",
    "    # loss = contrastive_loss(out, data.positive_pairs, data.negative_pairs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.702794Z",
     "end_time": "2023-04-11T10:40:27.713916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def test(model, data, device):\n",
    "    model.eval()\n",
    "    out = model(data.to(device))\n",
    "    loss = unsupervised_loss(out[data.test_mask], data.x[data.test_mask])\n",
    "    # loss = contrastive_loss(out, data.positive_pairs, data.negative_pairs)\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.706440Z",
     "end_time": "2023-04-11T10:40:27.714395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data, device, epochs):\n",
    "    best_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train(model, optimizer, data, device)\n",
    "        test_loss = test(model, data, device)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_weights = model.state_dict()\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Test Loss: {:.5f}'.format(epoch, loss, test_loss))\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.710579Z",
     "end_time": "2023-04-11T10:40:27.714605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def preprocess_data(networkx_graph, node_features):\n",
    "    \"\"\"\n",
    "    :Function: Preprocess\n",
    "    :param networkx_graph: Networkx graph\n",
    "    :param node_features: Node features\n",
    "    :return: data\n",
    "    :rtype: torch_geometric.data.Data\n",
    "    \"\"\"\n",
    "    # Convert to torch_geometric.data.Data\n",
    "    data = from_networkx(networkx_graph)\n",
    "\n",
    "    # Add node features\n",
    "    data.x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    # Add train and test mask\n",
    "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
    "    data.train_mask[:data.num_nodes // 2] = 1\n",
    "    data.test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
    "    data.test_mask[data.num_nodes // 2:] = 1\n",
    "    data.mapping = {node: i for i, node in enumerate(networkx_graph.nodes)}\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.714315Z",
     "end_time": "2023-04-11T10:40:27.722823Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded 0 stations\n",
      "twopi\n",
      "sfdp\n"
     ]
    }
   ],
   "source": [
    "networkGraphs = NetworkGraphs('../../datasets/Railway.csv', type=\"RAILWAY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:27.719211Z",
     "end_time": "2023-04-11T10:40:31.376944Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if choose to use one-hot features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "num_nodes = networkGraphs.Graph.number_of_nodes()\n",
    "one_hot_features = np.eye(num_nodes)\n",
    "data = preprocess_data(networkGraphs.Graph, one_hot_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T18:46:07.183561Z",
     "end_time": "2023-04-07T18:46:07.291734Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if choose to use metrics features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;94mCACHE: Computing value for compute_nodes_degree, hash: \u001B[0;93m31f0ab76850555acd3c40fcc94a42615 \n",
      "\u001B[0;94mCACHE: Computing value for compute_page_rank, hash: \u001B[0;93m7a98fdce508707fcc7885a16e218def5 \n",
      "\u001B[0;94mCACHE: Computing value for compute_kcore, hash: \u001B[0;93mfaa21788490ebd1e30b49e93b46ec8b4 \n",
      "\u001B[0;94mCACHE: Computing value for compute_triangles, hash: \u001B[0;93m6598f368c351016f17ad5184551605b1 \n"
     ]
    }
   ],
   "source": [
    "from src.metrics import get_metrics\n",
    "['degree', 'pagerank', 'kcore', 'triangles']\n",
    "features = []\n",
    "for metric in ['degree', 'pagerank', 'kcore', 'triangles']:\n",
    "    df = get_metrics(networkGraphs, metric, directed=False, multi=False)\n",
    "    np_arr = np.array(df.iloc[:, 1].values)\n",
    "    np_arr = (np_arr - np_arr.min()) / (np_arr.max() - np_arr.min())\n",
    "    features.append(np_arr)\n",
    "features = np.array(features).T\n",
    "data = preprocess_data(networkGraphs.Graph, features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:31.382025Z",
     "end_time": "2023-04-11T10:40:31.523144Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.08752, Test Loss: 0.08898\n",
      "Epoch: 002, Loss: 0.05818, Test Loss: 0.06162\n",
      "Epoch: 003, Loss: 0.03716, Test Loss: 0.04026\n",
      "Epoch: 004, Loss: 0.02202, Test Loss: 0.02576\n",
      "Epoch: 005, Loss: 0.01316, Test Loss: 0.01780\n",
      "Epoch: 006, Loss: 0.00998, Test Loss: 0.01466\n",
      "Epoch: 007, Loss: 0.01062, Test Loss: 0.01428\n",
      "Epoch: 008, Loss: 0.01297, Test Loss: 0.01503\n",
      "Epoch: 009, Loss: 0.01548, Test Loss: 0.01583\n",
      "Epoch: 010, Loss: 0.01721, Test Loss: 0.01599\n",
      "Epoch: 011, Loss: 0.01765, Test Loss: 0.01519\n",
      "Epoch: 012, Loss: 0.01668, Test Loss: 0.01352\n",
      "Epoch: 013, Loss: 0.01460, Test Loss: 0.01141\n",
      "Epoch: 014, Loss: 0.01190, Test Loss: 0.00931\n",
      "Epoch: 015, Loss: 0.00915, Test Loss: 0.00760\n",
      "Epoch: 016, Loss: 0.00675, Test Loss: 0.00650\n",
      "Epoch: 017, Loss: 0.00494, Test Loss: 0.00607\n",
      "Epoch: 018, Loss: 0.00381, Test Loss: 0.00624\n",
      "Epoch: 019, Loss: 0.00333, Test Loss: 0.00683\n",
      "Epoch: 020, Loss: 0.00336, Test Loss: 0.00760\n",
      "Epoch: 021, Loss: 0.00370, Test Loss: 0.00832\n",
      "Epoch: 022, Loss: 0.00417, Test Loss: 0.00878\n",
      "Epoch: 023, Loss: 0.00456, Test Loss: 0.00887\n",
      "Epoch: 024, Loss: 0.00475, Test Loss: 0.00854\n",
      "Epoch: 025, Loss: 0.00468, Test Loss: 0.00787\n",
      "Epoch: 026, Loss: 0.00435, Test Loss: 0.00698\n",
      "Epoch: 027, Loss: 0.00388, Test Loss: 0.00599\n",
      "Epoch: 028, Loss: 0.00334, Test Loss: 0.00498\n",
      "Epoch: 029, Loss: 0.00281, Test Loss: 0.00405\n",
      "Epoch: 030, Loss: 0.00236, Test Loss: 0.00327\n",
      "Epoch: 031, Loss: 0.00205, Test Loss: 0.00271\n",
      "Epoch: 032, Loss: 0.00190, Test Loss: 0.00238\n",
      "Epoch: 033, Loss: 0.00190, Test Loss: 0.00223\n",
      "Epoch: 034, Loss: 0.00198, Test Loss: 0.00218\n",
      "Epoch: 035, Loss: 0.00209, Test Loss: 0.00220\n",
      "Epoch: 036, Loss: 0.00219, Test Loss: 0.00222\n",
      "Epoch: 037, Loss: 0.00224, Test Loss: 0.00220\n",
      "Epoch: 038, Loss: 0.00221, Test Loss: 0.00211\n",
      "Epoch: 039, Loss: 0.00210, Test Loss: 0.00197\n",
      "Epoch: 040, Loss: 0.00192, Test Loss: 0.00182\n",
      "Epoch: 041, Loss: 0.00171, Test Loss: 0.00171\n",
      "Epoch: 042, Loss: 0.00152, Test Loss: 0.00164\n",
      "Epoch: 043, Loss: 0.00136, Test Loss: 0.00162\n",
      "Epoch: 044, Loss: 0.00126, Test Loss: 0.00166\n",
      "Epoch: 045, Loss: 0.00121, Test Loss: 0.00173\n",
      "Epoch: 046, Loss: 0.00122, Test Loss: 0.00181\n",
      "Epoch: 047, Loss: 0.00125, Test Loss: 0.00186\n",
      "Epoch: 048, Loss: 0.00128, Test Loss: 0.00188\n",
      "Epoch: 049, Loss: 0.00129, Test Loss: 0.00184\n",
      "Epoch: 050, Loss: 0.00126, Test Loss: 0.00175\n",
      "Epoch: 051, Loss: 0.00121, Test Loss: 0.00163\n",
      "Epoch: 052, Loss: 0.00113, Test Loss: 0.00150\n",
      "Epoch: 053, Loss: 0.00104, Test Loss: 0.00137\n",
      "Epoch: 054, Loss: 0.00096, Test Loss: 0.00128\n",
      "Epoch: 055, Loss: 0.00091, Test Loss: 0.00122\n",
      "Epoch: 056, Loss: 0.00089, Test Loss: 0.00119\n",
      "Epoch: 057, Loss: 0.00088, Test Loss: 0.00118\n",
      "Epoch: 058, Loss: 0.00089, Test Loss: 0.00117\n",
      "Epoch: 059, Loss: 0.00090, Test Loss: 0.00117\n",
      "Epoch: 060, Loss: 0.00090, Test Loss: 0.00115\n",
      "Epoch: 061, Loss: 0.00088, Test Loss: 0.00113\n",
      "Epoch: 062, Loss: 0.00085, Test Loss: 0.00110\n",
      "Epoch: 063, Loss: 0.00082, Test Loss: 0.00107\n",
      "Epoch: 064, Loss: 0.00078, Test Loss: 0.00105\n",
      "Epoch: 065, Loss: 0.00075, Test Loss: 0.00103\n",
      "Epoch: 066, Loss: 0.00073, Test Loss: 0.00103\n",
      "Epoch: 067, Loss: 0.00071, Test Loss: 0.00102\n",
      "Epoch: 068, Loss: 0.00071, Test Loss: 0.00102\n",
      "Epoch: 069, Loss: 0.00070, Test Loss: 0.00101\n",
      "Epoch: 070, Loss: 0.00069, Test Loss: 0.00100\n",
      "Epoch: 071, Loss: 0.00068, Test Loss: 0.00097\n",
      "Epoch: 072, Loss: 0.00067, Test Loss: 0.00094\n",
      "Epoch: 073, Loss: 0.00065, Test Loss: 0.00091\n",
      "Epoch: 074, Loss: 0.00063, Test Loss: 0.00088\n",
      "Epoch: 075, Loss: 0.00061, Test Loss: 0.00085\n",
      "Epoch: 076, Loss: 0.00059, Test Loss: 0.00082\n",
      "Epoch: 077, Loss: 0.00058, Test Loss: 0.00079\n",
      "Epoch: 078, Loss: 0.00057, Test Loss: 0.00077\n",
      "Epoch: 079, Loss: 0.00056, Test Loss: 0.00075\n",
      "Epoch: 080, Loss: 0.00055, Test Loss: 0.00074\n",
      "Epoch: 081, Loss: 0.00054, Test Loss: 0.00072\n",
      "Epoch: 082, Loss: 0.00053, Test Loss: 0.00070\n",
      "Epoch: 083, Loss: 0.00052, Test Loss: 0.00068\n",
      "Epoch: 084, Loss: 0.00051, Test Loss: 0.00066\n",
      "Epoch: 085, Loss: 0.00049, Test Loss: 0.00064\n",
      "Epoch: 086, Loss: 0.00048, Test Loss: 0.00063\n",
      "Epoch: 087, Loss: 0.00047, Test Loss: 0.00061\n",
      "Epoch: 088, Loss: 0.00046, Test Loss: 0.00060\n",
      "Epoch: 089, Loss: 0.00046, Test Loss: 0.00059\n",
      "Epoch: 090, Loss: 0.00045, Test Loss: 0.00057\n",
      "Epoch: 091, Loss: 0.00044, Test Loss: 0.00056\n",
      "Epoch: 092, Loss: 0.00043, Test Loss: 0.00055\n",
      "Epoch: 093, Loss: 0.00042, Test Loss: 0.00053\n",
      "Epoch: 094, Loss: 0.00041, Test Loss: 0.00052\n",
      "Epoch: 095, Loss: 0.00040, Test Loss: 0.00050\n",
      "Epoch: 096, Loss: 0.00039, Test Loss: 0.00049\n",
      "Epoch: 097, Loss: 0.00039, Test Loss: 0.00047\n",
      "Epoch: 098, Loss: 0.00038, Test Loss: 0.00046\n",
      "Epoch: 099, Loss: 0.00037, Test Loss: 0.00045\n",
      "Epoch: 100, Loss: 0.00036, Test Loss: 0.00044\n",
      "Epoch: 101, Loss: 0.00036, Test Loss: 0.00043\n",
      "Epoch: 102, Loss: 0.00035, Test Loss: 0.00042\n",
      "Epoch: 103, Loss: 0.00034, Test Loss: 0.00041\n",
      "Epoch: 104, Loss: 0.00033, Test Loss: 0.00040\n",
      "Epoch: 105, Loss: 0.00033, Test Loss: 0.00039\n",
      "Epoch: 106, Loss: 0.00032, Test Loss: 0.00039\n",
      "Epoch: 107, Loss: 0.00031, Test Loss: 0.00038\n",
      "Epoch: 108, Loss: 0.00031, Test Loss: 0.00037\n",
      "Epoch: 109, Loss: 0.00030, Test Loss: 0.00036\n",
      "Epoch: 110, Loss: 0.00030, Test Loss: 0.00035\n",
      "Epoch: 111, Loss: 0.00029, Test Loss: 0.00035\n",
      "Epoch: 112, Loss: 0.00029, Test Loss: 0.00034\n",
      "Epoch: 113, Loss: 0.00028, Test Loss: 0.00033\n",
      "Epoch: 114, Loss: 0.00027, Test Loss: 0.00032\n",
      "Epoch: 115, Loss: 0.00027, Test Loss: 0.00031\n",
      "Epoch: 116, Loss: 0.00026, Test Loss: 0.00031\n",
      "Epoch: 117, Loss: 0.00026, Test Loss: 0.00030\n",
      "Epoch: 118, Loss: 0.00025, Test Loss: 0.00029\n",
      "Epoch: 119, Loss: 0.00025, Test Loss: 0.00029\n",
      "Epoch: 120, Loss: 0.00024, Test Loss: 0.00028\n",
      "Epoch: 121, Loss: 0.00024, Test Loss: 0.00028\n",
      "Epoch: 122, Loss: 0.00023, Test Loss: 0.00027\n",
      "Epoch: 123, Loss: 0.00023, Test Loss: 0.00026\n",
      "Epoch: 124, Loss: 0.00022, Test Loss: 0.00026\n",
      "Epoch: 125, Loss: 0.00022, Test Loss: 0.00025\n",
      "Epoch: 126, Loss: 0.00022, Test Loss: 0.00025\n",
      "Epoch: 127, Loss: 0.00021, Test Loss: 0.00024\n",
      "Epoch: 128, Loss: 0.00021, Test Loss: 0.00023\n",
      "Epoch: 129, Loss: 0.00020, Test Loss: 0.00023\n",
      "Epoch: 130, Loss: 0.00020, Test Loss: 0.00022\n",
      "Epoch: 131, Loss: 0.00020, Test Loss: 0.00022\n",
      "Epoch: 132, Loss: 0.00019, Test Loss: 0.00021\n",
      "Epoch: 133, Loss: 0.00019, Test Loss: 0.00021\n",
      "Epoch: 134, Loss: 0.00019, Test Loss: 0.00020\n",
      "Epoch: 135, Loss: 0.00018, Test Loss: 0.00020\n",
      "Epoch: 136, Loss: 0.00018, Test Loss: 0.00020\n",
      "Epoch: 137, Loss: 0.00017, Test Loss: 0.00019\n",
      "Epoch: 138, Loss: 0.00017, Test Loss: 0.00019\n",
      "Epoch: 139, Loss: 0.00017, Test Loss: 0.00018\n",
      "Epoch: 140, Loss: 0.00017, Test Loss: 0.00018\n",
      "Epoch: 141, Loss: 0.00016, Test Loss: 0.00018\n",
      "Epoch: 142, Loss: 0.00016, Test Loss: 0.00017\n",
      "Epoch: 143, Loss: 0.00016, Test Loss: 0.00017\n",
      "Epoch: 144, Loss: 0.00015, Test Loss: 0.00016\n",
      "Epoch: 145, Loss: 0.00015, Test Loss: 0.00016\n",
      "Epoch: 146, Loss: 0.00015, Test Loss: 0.00016\n",
      "Epoch: 147, Loss: 0.00015, Test Loss: 0.00015\n",
      "Epoch: 148, Loss: 0.00014, Test Loss: 0.00015\n",
      "Epoch: 149, Loss: 0.00014, Test Loss: 0.00015\n",
      "Epoch: 150, Loss: 0.00014, Test Loss: 0.00015\n",
      "Epoch: 151, Loss: 0.00013, Test Loss: 0.00014\n",
      "Epoch: 152, Loss: 0.00013, Test Loss: 0.00014\n",
      "Epoch: 153, Loss: 0.00013, Test Loss: 0.00014\n",
      "Epoch: 154, Loss: 0.00013, Test Loss: 0.00013\n",
      "Epoch: 155, Loss: 0.00013, Test Loss: 0.00013\n",
      "Epoch: 156, Loss: 0.00012, Test Loss: 0.00013\n",
      "Epoch: 157, Loss: 0.00012, Test Loss: 0.00013\n",
      "Epoch: 158, Loss: 0.00012, Test Loss: 0.00012\n",
      "Epoch: 159, Loss: 0.00012, Test Loss: 0.00012\n",
      "Epoch: 160, Loss: 0.00011, Test Loss: 0.00012\n",
      "Epoch: 161, Loss: 0.00011, Test Loss: 0.00012\n",
      "Epoch: 162, Loss: 0.00011, Test Loss: 0.00012\n",
      "Epoch: 163, Loss: 0.00011, Test Loss: 0.00011\n",
      "Epoch: 164, Loss: 0.00011, Test Loss: 0.00011\n",
      "Epoch: 165, Loss: 0.00011, Test Loss: 0.00011\n",
      "Epoch: 166, Loss: 0.00010, Test Loss: 0.00011\n",
      "Epoch: 167, Loss: 0.00010, Test Loss: 0.00010\n",
      "Epoch: 168, Loss: 0.00010, Test Loss: 0.00010\n",
      "Epoch: 169, Loss: 0.00010, Test Loss: 0.00010\n",
      "Epoch: 170, Loss: 0.00010, Test Loss: 0.00010\n",
      "Epoch: 171, Loss: 0.00010, Test Loss: 0.00010\n",
      "Epoch: 172, Loss: 0.00009, Test Loss: 0.00010\n",
      "Epoch: 173, Loss: 0.00009, Test Loss: 0.00009\n",
      "Epoch: 174, Loss: 0.00009, Test Loss: 0.00009\n",
      "Epoch: 175, Loss: 0.00009, Test Loss: 0.00009\n",
      "Epoch: 176, Loss: 0.00009, Test Loss: 0.00009\n",
      "Epoch: 177, Loss: 0.00009, Test Loss: 0.00009\n",
      "Epoch: 178, Loss: 0.00008, Test Loss: 0.00009\n",
      "Epoch: 179, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 180, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 181, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 182, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 183, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 184, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 185, Loss: 0.00008, Test Loss: 0.00008\n",
      "Epoch: 186, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 187, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 188, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 189, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 190, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 191, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 192, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 193, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 194, Loss: 0.00007, Test Loss: 0.00007\n",
      "Epoch: 195, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 196, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 197, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 198, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 199, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 200, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 201, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 202, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 203, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 204, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 205, Loss: 0.00006, Test Loss: 0.00006\n",
      "Epoch: 206, Loss: 0.00006, Test Loss: 0.00005\n",
      "Epoch: 207, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 208, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 209, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 210, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 211, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 212, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 213, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 214, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 215, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 216, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 217, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 218, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 219, Loss: 0.00005, Test Loss: 0.00005\n",
      "Epoch: 220, Loss: 0.00005, Test Loss: 0.00004\n",
      "Epoch: 221, Loss: 0.00005, Test Loss: 0.00004\n",
      "Epoch: 222, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 223, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 224, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 225, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 226, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 227, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 228, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 229, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 230, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 231, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 232, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 233, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 234, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 235, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 236, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 237, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 238, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 239, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 240, Loss: 0.00004, Test Loss: 0.00004\n",
      "Epoch: 241, Loss: 0.00004, Test Loss: 0.00003\n",
      "Epoch: 242, Loss: 0.00004, Test Loss: 0.00003\n",
      "Epoch: 243, Loss: 0.00004, Test Loss: 0.00003\n",
      "Epoch: 244, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 245, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 246, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 247, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 248, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 249, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 250, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 251, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 252, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 253, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 254, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 255, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 256, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 257, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 258, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 259, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 260, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 261, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 262, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 263, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 264, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 265, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 266, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 267, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 268, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 269, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 270, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 271, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 272, Loss: 0.00003, Test Loss: 0.00003\n",
      "Epoch: 273, Loss: 0.00003, Test Loss: 0.00002\n",
      "Epoch: 274, Loss: 0.00003, Test Loss: 0.00002\n",
      "Epoch: 275, Loss: 0.00003, Test Loss: 0.00002\n",
      "Epoch: 276, Loss: 0.00003, Test Loss: 0.00002\n",
      "Epoch: 277, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 278, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 279, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 280, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 281, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 282, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 283, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 284, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 285, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 286, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 287, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 288, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 289, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 290, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 291, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 292, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 293, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 294, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 295, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 296, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 297, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 298, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 299, Loss: 0.00002, Test Loss: 0.00002\n",
      "Epoch: 300, Loss: 0.00002, Test Loss: 0.00002\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 256\n",
    "embed_dim = data.num_features\n",
    "gae_model = GAE2(data.num_features, hidden_dim, embed_dim).to(device)\n",
    "optimizer = torch.optim.Adam(gae_model.parameters(), lr=0.001)\n",
    "epochs = 300\n",
    "positive_pairs, negative_pairs = generate_pairs(networkGraphs.Graph, num_negative_pairs=1000)\n",
    "positive_indices, negative_indices= pairs_to_indices(data, positive_pairs, negative_pairs)\n",
    "data.positive_pairs = positive_indices\n",
    "data.negative_pairs = negative_indices\n",
    "gae_model = train_model(gae_model, optimizer, data, device, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:31.526365Z",
     "end_time": "2023-04-11T10:40:41.661861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "gae_model.eval()\n",
    "with torch.no_grad():\n",
    "    x, edge_index = data.x, data.edge_index\n",
    "    embeddings = gae_model.encoder(x, edge_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:43.031675Z",
     "end_time": "2023-04-11T10:40:43.045626Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2719, 256])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:40:46.395679Z",
     "end_time": "2023-04-11T10:40:46.404371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2719, 256])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = get_communities(networkGraphs, method='kmeans', noOfClusters=4, embedding=embeddings)\n",
    "TSNE_visualisation(networkGraphs, embeddings, filename='TSNE_GAE.html', clusters=clusters)\n",
    "generate_static_cluster(networkGraphs, clusters, 'sfdp.html', 'kmeans', layout_='map', nbr=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:29:24.842060Z",
     "end_time": "2023-04-11T10:29:24.857620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(\n  pos=[2719, 2],\n  start=[12338],\n  end=[12338],\n  color=[12338],\n  weight=[12338],\n  x=[2719, 4],\n  train_mask=[2719],\n  test_mask=[2719],\n  mapping={\n    692=0,\n    1351=1,\n    698=2,\n    944=3,\n    232=4,\n    1639=5,\n    2335=6,\n    1512=7,\n    127=8,\n    259=9,\n    1835=10,\n    1480=11,\n    2362=12,\n    503=13,\n    1009=14,\n    2058=15,\n    802=16,\n    681=17,\n    1704=18,\n    2314=19,\n    314=20,\n    783=21,\n    542=22,\n    1148=23,\n    683=24,\n    983=25,\n    390=26,\n    804=27,\n    1005=28,\n    1309=29,\n    627=30,\n    123=31,\n    2330=32,\n    529=33,\n    530=34,\n    826=35,\n    2007=36,\n    381=37,\n    382=38,\n    158=39,\n    202=40,\n    131=41,\n    903=42,\n    160=43,\n    161=44,\n    2140=45,\n    92=46,\n    1146=47,\n    2168=48,\n    1030=49,\n    1786=50,\n    2030=51,\n    561=52,\n    752=53,\n    1384=54,\n    317=55,\n    997=56,\n    2175=57,\n    1508=58,\n    1580=59,\n    2020=60,\n    1254=61,\n    1625=62,\n    1633=63,\n    47=64,\n    1573=65,\n    2441=66,\n    1256=67,\n    1046=68,\n    2245=69,\n    957=70,\n    806=71,\n    1942=72,\n    891=73,\n    502=74,\n    1554=75,\n    2282=76,\n    229=77,\n    413=78,\n    46=79,\n    148=80,\n    898=81,\n    560=82,\n    269=83,\n    1899=84,\n    1058=85,\n    1620=86,\n    1122=87,\n    2433=88,\n    2445=89,\n    1521=90,\n    2023=91,\n    1377=92,\n    177=93,\n    514=94,\n    2105=95,\n    2477=96,\n    1481=97,\n    1066=98,\n    99=99,\n    1561=100,\n    504=101,\n    2281=102,\n    1569=103,\n    147=104,\n    11=105,\n    2234=106,\n    2012=107,\n    328=108,\n    1885=109,\n    867=110,\n    1833=111,\n    1603=112,\n    558=113,\n    2357=114,\n    1474=115,\n    1040=116,\n    1517=117,\n    1498=118,\n    288=119,\n    1599=120,\n    812=121,\n    2032=122,\n    1595=123,\n    420=124,\n    141=125,\n    913=126,\n    1163=127,\n    1963=128,\n    57=129,\n    2240=130,\n    1286=131,\n    1827=132,\n    924=133,\n    523=134,\n    79=135,\n    1488=136,\n    1250=137,\n    1562=138,\n    2038=139,\n    1567=140,\n    1923=141,\n    1999=142,\n    1213=143,\n    786=144,\n    2185=145,\n    2176=146,\n    1336=147,\n    493=148,\n    2293=149,\n    134=150,\n    240=151,\n    2339=152,\n    2152=153,\n    253=154,\n    90=155,\n    291=156,\n    705=157,\n    1903=158,\n    582=159,\n    2108=160,\n    1874=161,\n    1875=162,\n    578=163,\n    2322=164,\n    1054=165,\n    1178=166,\n    387=167,\n    426=168,\n    980=169,\n    1462=170,\n    686=171,\n    2295=172,\n    415=173,\n    1604=174,\n    389=175,\n    1248=176,\n    684=177,\n    2340=178,\n    1796=179,\n    2172=180,\n    1171=181,\n    1181=182,\n    2137=183,\n    1407=184,\n    1714=185,\n    2284=186,\n    1189=187,\n    1431=188,\n    1893=189,\n    1047=190,\n    1070=191,\n    1805=192,\n    2372=193,\n    1287=194,\n    1925=195,\n    1943=196,\n    666=197,\n    1455=198,\n    2218=199,\n    995=200,\n    1176=201,\n    300=202,\n    2259=203,\n    1041=204,\n    548=205,\n    528=206,\n    339=207,\n    1886=208,\n    963=209,\n    2216=210,\n    1220=211,\n    434=212,\n    1436=213,\n    257=214,\n    2373=215,\n    1522=216,\n    580=217,\n    1473=218,\n    618=219,\n    2447=220,\n    665=221,\n    1352=222,\n    1365=223,\n    1446=224,\n    2267=225,\n    2268=226,\n    2396=227,\n    1195=228,\n    1218=229,\n    1646=230,\n    1115=231,\n    1551=232,\n    1617=233,\n    2343=234,\n    1062=235,\n    1766=236,\n    2226=237,\n    2350=238,\n    115=239,\n    109=240,\n    1208=241,\n    1743=242,\n    355=243,\n    1048=244,\n    948=245,\n    694=246,\n    962=247,\n    895=248,\n    892=249,\n    777=250,\n    183=251,\n    1528=252,\n    993=253,\n    631=254,\n    1150=255,\n    378=256,\n    700=257,\n    1837=258,\n    1120=259,\n    1279=260,\n    2227=261,\n    637=262,\n    2469=263,\n    151=264,\n    925=265,\n    1718=266,\n    1514=267,\n    998=268,\n    1102=269,\n    803=270,\n    2398=271,\n    1000=272,\n    1294=273,\n    1275=274,\n    644=275,\n    1281=276,\n    1880=277,\n    1328=278,\n    1363=279,\n    386=280,\n    565=281,\n    2010=282,\n    1006=283,\n    509=284,\n    2439=285,\n    1053=286,\n    247=287,\n    2353=288,\n    2154=289,\n    1656=290,\n    36=291,\n    2034=292,\n    2035=293,\n    1300=294,\n    1243=295,\n    2459=296,\n    431=297,\n    589=298,\n    985=299,\n    757=300,\n    1104=301,\n    2033=302,\n    1937=303,\n    866=304,\n    208=305,\n    1092=306,\n    933=307,\n    734=308,\n    2374=309,\n    1676=310,\n    671=311,\n    2207=312,\n    1201=313,\n    1694=314,\n    1649=315,\n    1757=316,\n    658=317,\n    2359=318,\n    1456=319,\n    1082=320,\n    856=321,\n    222=322,\n    1358=323,\n    865=324,\n    1536=325,\n    1661=326,\n    1954=327,\n    1671=328,\n    2301=329,\n    473=330,\n    2129=331,\n    1529=332,\n    1959=333,\n    297=334,\n    477=335,\n    1501=336,\n    1429=337,\n    822=338,\n    846=339,\n    191=340,\n    584=341,\n    1091=342,\n    304=343,\n    2076=344,\n    1917=345,\n    244=346,\n    174=347,\n    2423=348,\n    727=349,\n    556=350,\n    545=351,\n    1864=352,\n    2132=353,\n    643=354,\n    163=355,\n    1226=356,\n    375=357,\n    280=358,\n    1280=359,\n    1631=360,\n    1004=361,\n    1085=362,\n    69=363,\n    937=364,\n    1251=365,\n    23=366,\n    353=367,\n    498=368,\n    303=369,\n    1227=370,\n    400=371,\n    296=372,\n    352=373,\n    1144=374,\n    838=375,\n    1084=376,\n    896=377,\n    575=378,\n    1519=379,\n    1972=380,\n    264=381,\n    2465=382,\n    2242=383,\n    1073=384,\n    455=385,\n    155=386,\n    286=387,\n    1655=388,\n    673=389,\n    1074=390,\n    1534=391,\n    544=392,\n    938=393,\n    1763=394,\n    1136=395,\n    1659=396,\n    2067=397,\n    549=398,\n    1200=399,\n    911=400,\n    2367=401,\n    814=402,\n    337=403,\n    2427=404,\n    1055=405,\n    258=406,\n    485=407,\n    284=408,\n    283=409,\n    393=410,\n    2111=411,\n    2148=412,\n    1524=413,\n    1888=414,\n    1231=415,\n    2153=416,\n    354=417,\n    5=418,\n    1438=419,\n    215=420,\n    137=421,\n    1277=422,\n    1984=423,\n    446=424,\n    581=425,\n    1011=426,\n    103=427,\n    1077=428,\n    1525=429,\n    1238=430,\n    605=431,\n    1164=432,\n    1990=433,\n    130=434,\n    2097=435,\n    945=436,\n    1491=437,\n    1211=438,\n    89=439,\n    2251=440,\n    176=441,\n    507=442,\n    1444=443,\n    623=444,\n    976=445,\n    678=446,\n    1188=447,\n    2431=448,\n    1112=449,\n    2380=450,\n    77=451,\n    745=452,\n    2079=453,\n    346=454,\n    1258=455,\n    1500=456,\n    2151=457,\n    1147=458,\n    1806=459,\n    1024=460,\n    689=461,\n    1049=462,\n    465=463,\n    749=464,\n    1219=465,\n    877=466,\n    1051=467,\n    2112=468,\n    1180=469,\n    557=470,\n    1653=471,\n    1699=472,\n    1086=473,\n    611=474,\n    2382=475,\n    396=476,\n    1552=477,\n    1544=478,\n    1556=479,\n    1416=480,\n    1224=481,\n    1916=482,\n    1926=483,\n    1076=484,\n    1601=485,\n    889=486,\n    635=487,\n    767=488,\n    2177=489,\n    1137=490,\n    642=491,\n    1186=492,\n    2075=493,\n    374=494,\n    1261=495,\n    1915=496,\n    842=497,\n    489=498,\n    640=499,\n    377=500,\n    1128=501,\n    702=502,\n    610=503,\n    1278=504,\n    1142=505,\n    620=506,\n    1790=507,\n    1914=508,\n    606=509,\n    609=510,\n    1883=511,\n    1696=512,\n    1761=513,\n    2305=514,\n    590=515,\n    1610=516,\n    1155=517,\n    2346=518,\n    1866=519,\n    691=520,\n    2120=521,\n    2044=522,\n    1075=523,\n    2061=524,\n    331=525,\n    904=526,\n    1068=527,\n    2479=528,\n    1721=529,\n    2025=530,\n    819=531,\n    2375=532,\n    363=533,\n    108=534,\n    2272=535,\n    2141=536,\n    1793=537,\n    441=538,\n    2321=539,\n    1836=540,\n    246=541,\n    67=542,\n    2378=543,\n    1865=544,\n    1890=545,\n    1889=546,\n    4=547,\n    111=548,\n    820=549,\n    1913=550,\n    547=551,\n    981=552,\n    1720=553,\n    185=554,\n    192=555,\n    205=556,\n    2486=557,\n    2047=558,\n    2124=559,\n    733=560,\n    1622=561,\n    2060=562,\n    2235=563,\n    2466=564,\n    1765=565,\n    1310=566,\n    1232=567,\n    798=568,\n    1344=569,\n    1276=570,\n    2073=571,\n    312=572,\n    2417=573,\n    2415=574,\n    1398=575,\n    815=576,\n    2462=577,\n    93=578,\n    1609=579,\n    668=580,\n    2723=581,\n    2722=582,\n    1799=583,\n    239=584,\n    398=585,\n    298=586,\n    1342=587,\n    1781=588,\n    1305=589,\n    772=590,\n    2391=591,\n    349=592,\n    551=593,\n    292=594,\n    1056=595,\n    1015=596,\n    636=597,\n    13=598,\n    2487=599,\n    624=600,\n    1440=601,\n    48=602,\n    1348=603,\n    1411=604,\n    583=605,\n    695=606,\n    117=607,\n    1843=608,\n    1817=609,\n    408=610,\n    1857=611,\n    1123=612,\n    178=613,\n    823=614,\n    515=615,\n    479=616,\n    1098=617,\n    371=618,\n    988=619,\n    478=620,\n    1606=621,\n    1335=622,\n    209=623,\n    517=624,\n    486=625,\n    224=626,\n    2341=627,\n    2026=628,\n    255=629,\n    2149=630,\n    379=631,\n    1241=632,\n    271=633,\n    2277=634,\n    1334=635,\n    295=636,\n    740=637,\n    1709=638,\n    2249=639,\n    667=640,\n    1414=641,\n    1662=642,\n    1647=643,\n    156=644,\n    874=645,\n    466=646,\n    971=647,\n    876=648,\n    875=649,\n    591=650,\n    241=651,\n    555=652,\n    1314=653,\n    1222=654,\n    2210=655,\n    1660=656,\n    2309=657,\n    2296=658,\n    1600=659,\n    154=660,\n    773=661,\n    2110=662,\n    1930=663,\n    1996=664,\n    2426=665,\n    1121=666,\n    886=667,\n    2424=668,\n    410=669,\n    1330=670,\n    2199=671,\n    2669=672,\n    1778=673,\n    1379=674,\n    2659=675,\n    2812=676,\n    2719=677,\n    2749=678,\n    2767=679,\n    521=680,\n    2316=681,\n    712=682,\n    717=683,\n    72=684,\n    2003=685,\n    1919=686,\n    1635=687,\n    2147=688,\n    2418=689,\n    1998=690,\n    2159=691,\n    1918=692,\n    1722=693,\n    533=694,\n    1433=695,\n    619=696,\n    1589=697,\n    1263=698,\n    1386=699,\n    233=700,\n    1266=701,\n    1487=702,\n    1788=703,\n    1734=704,\n    2338=705,\n    2625=706,\n    2713=707,\n    2748=708,\n    2528=709,\n    2531=710,\n    2527=711,\n    2783=712,\n    2502=713,\n    2683=714,\n    2732=715,\n    2597=716,\n    2778=717,\n    2653=718,\n    2591=719,\n    2652=720,\n    2781=721,\n    2782=722,\n    2716=723,\n    2548=724,\n    2524=725,\n    2517=726,\n    2558=727,\n    2602=728,\n    2627=729,\n    2786=730,\n    2546=731,\n    2660=732,\n    2728=733,\n    1343=734,\n    220=735,\n    626=736,\n    120=737,\n    508=738,\n    1847=739,\n    1451=740,\n    908=741,\n    2130=742,\n    714=743,\n    210=744,\n    362=745,\n    1350=746,\n    2681=747,\n    2734=748,\n    630=749,\n    1769=750,\n    2215=751,\n    2126=752,\n    256=753,\n    978=754,\n    1628=755,\n    679=756,\n    2328=757,\n    674=758,\n    206=759,\n    1329=760,\n    484=761,\n    2146=762,\n    1957=763,\n    2416=764,\n    680=765,\n    2270=766,\n    1408=767,\n    810=768,\n    2484=769,\n    1829=770,\n    2706=771,\n    1468=772,\n    2655=773,\n    676=774,\n    193=775,\n    1420=776,\n    145=777,\n    931=778,\n    932=779,\n    1644=780,\n    274=781,\n    2377=782,\n    2493=783,\n    2539=784,\n    2697=785,\n    2765=786,\n    2619=787,\n    2552=788,\n    2494=789,\n    2694=790,\n    2776=791,\n    2811=792,\n    2701=793,\n    2498=794,\n    2583=795,\n    2495=796,\n    2529=797,\n    2542=798,\n    2801=799,\n    2743=800,\n    2724=801,\n    2756=802,\n    2712=803,\n    2593=804,\n    2730=805,\n    2629=806,\n    2612=807,\n    2764=808,\n    1738=809,\n    2448=810,\n    2451=811,\n    656=812,\n    2560=813,\n    2515=814,\n    2789=815,\n    1225=816,\n    1016=817,\n    657=818,\n    2641=819,\n    1036=820,\n    1851=821,\n    1183=822,\n    203=823,\n    2658=824,\n    1737=825,\n    2656=826,\n    2508=827,\n    2303=828,\n    104=829,\n    1391=830,\n    2735=831,\n    2559=832,\n    2704=833,\n    2610=834,\n    2579=835,\n    2757=836,\n    2780=837,\n    2588=838,\n    2621=839,\n    2718=840,\n    2622=841,\n    2707=842,\n    645=843,\n    2522=844,\n    2201=845,\n    37=846,\n    2678=847,\n    2680=848,\n    1773=849,\n    2497=850,\n    2128=851,\n    2747=852,\n    74=853,\n    1340=854,\n    1307=855,\n    2386=856,\n    213=857,\n    40=858,\n    2736=859,\n    2543=860,\n    2537=861,\n    2599=862,\n    2787=863,\n    2740=864,\n    893=865,\n    369=866,\n    85=867,\n    2769=868,\n    2775=869,\n    2577=870,\n    31=871,\n    1113=872,\n    2008=873,\n    2326=874,\n    71=875,\n    1463=876,\n    776=877,\n    1165=878,\n    1485=879,\n    1686=880,\n    2167=881,\n    1467=882,\n    2164=883,\n    1771=884,\n    1196=885,\n    2123=886,\n    368=887,\n    2471=888,\n    1979=889,\n    2291=890,\n    219=891,\n    2024=892,\n    432=893,\n    2623=894,\n    1563=895,\n    1568=896,\n    1179=897,\n    778=898,\n    392=899,\n    1515=900,\n    430=901,\n    214=902,\n    190=903,\n    1010=904,\n    2209=905,\n    1605=906,\n    1337=907,\n    1724=908,\n    2115=909,\n    2784=910,\n    2572=911,\n    2695=912,\n    2405=913,\n    979=914,\n    1371=915,\n    1795=916,\n    2294=917,\n    218=918,\n    1541=919,\n    1125=920,\n    409=921,\n    1003=922,\n    2288=923,\n    105=924,\n    2274=925,\n    934=926,\n    1809=927,\n    2089=928,\n    940=929,\n    2397=930,\n    690=931,\n    1565=932,\n    226=933,\n    1110=934,\n    1445=935,\n    518=936,\n    1301=937,\n    188=938,\n    2468=939,\n    2104=940,\n    1234=941,\n    594=942,\n    1744=943,\n    1423=944,\n    449=945,\n    427=946,\n    2410=947,\n    947=948,\n    1507=949,\n    2162=950,\n    792=951,\n    2273=952,\n    899=953,\n    989=954,\n    1057=955,\n    766=956,\n    1419=957,\n    1780=958,\n    2051=959,\n    603=960,\n    1415=961,\n    1199=962,\n    41=963,\n    1980=964,\n    1897=965,\n    2399=966,\n    1545=967,\n    101=968,\n    456=969,\n    2018=970,\n    1994=971,\n    1540=972,\n    1320=973,\n    1641=974,\n    1932=975,\n    2083=976,\n    211=977,\n    1991=978,\n    1450=979,\n    1449=980,\n    2342=981,\n    194=982,\n    2069=983,\n    1691=984,\n    2262=985,\n    347=986,\n    53=987,\n    2476=988,\n    2325=989,\n    598=990,\n    1958=991,\n    1615=992,\n    1316=993,\n    1317=994,\n    2430=995,\n    2093=996,\n    323=997,\n    1670=998,\n    2311=999,\n    1107=1000,\n    1478=1001,\n    385=1002,\n    1971=1003,\n    2596=1004,\n    2595=1005,\n    1083=1006,\n    2677=1007,\n    2676=1008,\n    2608=1009,\n    888=1010,\n    2731=1011,\n    2814=1012,\n    2630=1013,\n    748=1014,\n    2760=1015,\n    358=1016,\n    2200=1017,\n    791=1018,\n    982=1019,\n    2157=1020,\n    1586=1021,\n    2192=1022,\n    2254=1023,\n    1547=1024,\n    1800=1025,\n    1735=1026,\n    1730=1027,\n    1746=1028,\n    2404=1029,\n    1021=1030,\n    263=1031,\n    2173=1032,\n    919=1033,\n    1313=1034,\n    2467=1035,\n    2409=1036,\n    2269=1037,\n    1489=1038,\n    2187=1039,\n    2480=1040,\n    1828=1041,\n    112=1042,\n    1378=1043,\n    1652=1044,\n    1470=1045,\n    2122=1046,\n    364=1047,\n    238=1048,\n    1252=1049,\n    318=1050,\n    884=1051,\n    1290=1052,\n    2198=1053,\n    321=1054,\n    1418=1055,\n    860=1056,\n    1964=1057,\n    351=1058,\n    602=1059,\n    433=1060,\n    2302=1061,\n    1725=1062,\n    1739=1063,\n    2116=1064,\n    2220=1065,\n    68=1066,\n    1093=1067,\n    1206=1068,\n    1702=1069,\n    2194=1070,\n    1975=1071,\n    834=1072,\n    12=1073,\n    1385=1074,\n    615=1075,\n    1855=1076,\n    38=1077,\n    2241=1078,\n    2114=1079,\n    1784=1080,\n    1229=1081,\n    1259=1082,\n    21=1083,\n    2014=1084,\n    697=1085,\n    179=1086,\n    648=1087,\n    1126=1088,\n    276=1089,\n    1326=1090,\n    1873=1091,\n    722=1092,\n    1413=1093,\n    1260=1094,\n    2160=1095,\n    2193=1096,\n    342=1097,\n    341=1098,\n    633=1099,\n    1935=1100,\n    265=1101,\n    732=1102,\n    1678=1103,\n    1106=1104,\n    2=1105,\n    790=1106,\n    789=1107,\n    1388=1108,\n    1356=1109,\n    832=1110,\n    2225=1111,\n    1608=1112,\n    1204=1113,\n    126=1114,\n    1755=1115,\n    1756=1116,\n    2088=1117,\n    1465=1118,\n    833=1119,\n    872=1120,\n    622=1121,\n    1215=1122,\n    1956=1123,\n    2161=1124,\n    1706=1125,\n    1019=1126,\n    1018=1127,\n    189=1128,\n    1190=1129,\n    1192=1130,\n    1668=1131,\n    1904=1132,\n    2348=1133,\n    2243=1134,\n    1157=1135,\n    2446=1136,\n    800=1137,\n    373=1138,\n    1324=1139,\n    360=1140,\n    2438=1141,\n    1822=1142,\n    2092=1143,\n    2096=1144,\n    2394=1145,\n    1135=1146,\n    771=1147,\n    384=1148,\n    306=1149,\n    320=1150,\n    1602=1151,\n    308=1152,\n    1985=1153,\n    2145=1154,\n    1950=1155,\n    1191=1156,\n    1977=1157,\n    164=1158,\n    162=1159,\n    1684=1160,\n    1779=1161,\n    1469=1162,\n    707=1163,\n    1850=1164,\n    454=1165,\n    2304=1166,\n    1849=1167,\n    859=1168,\n    906=1169,\n    242=1170,\n    243=1171,\n    2333=1172,\n    1770=1173,\n    311=1174,\n    1578=1175,\n    1537=1176,\n    458=1177,\n    927=1178,\n    1242=1179,\n    760=1180,\n    1929=1181,\n    1657=1182,\n    526=1183,\n    847=1184,\n    730=1185,\n    2022=1186,\n    1479=1187,\n    1437=1188,\n    1461=1189,\n    1002=1190,\n    83=1191,\n    975=1192,\n    1934=1193,\n    956=1194,\n    2228=1195,\n    2442=1196,\n    201=1197,\n    1245=1198,\n    110=1199,\n    1658=1200,\n    576=1201,\n    32=1202,\n    1214=1203,\n    391=1204,\n    2109=1205,\n    217=1206,\n    435=1207,\n    725=1208,\n    828=1209,\n    1944=1210,\n    2045=1211,\n    743=1212,\n    604=1213,\n    1130=1214,\n    1871=1215,\n    595=1216,\n    1965=1217,\n    294=1218,\n    402=1219,\n    2379=1220,\n    711=1221,\n    416=1222,\n    1825=1223,\n    43=1224,\n    1159=1225,\n    1383=1226,\n    2320=1227,\n    1911=1228,\n    302=1229,\n    80=1230,\n    2318=1231,\n    1674=1232,\n    1747=1233,\n    2183=1234,\n    480=1235,\n    2006=1236,\n    1976=1237,\n    30=1238,\n    2331=1239,\n    1269=1240,\n    1292=1241,\n    1216=1242,\n    1759=1243,\n    939=1244,\n    1951=1245,\n    1466=1246,\n    1868=1247,\n    920=1248,\n    821=1249,\n    1906=1250,\n    2119=1251,\n    1728=1252,\n    936=1253,\n    1729=1254,\n    1212=1255,\n    2383=1256,\n    520=1257,\n    1648=1258,\n    843=1259,\n    1249=1260,\n    836=1261,\n    2212=1262,\n    1172=1263,\n    1223=1264,\n    651=1265,\n    1650=1266,\n    894=1267,\n    2401=1268,\n    171=1269,\n    1357=1270,\n    1027=1271,\n    2381=1272,\n    1858=1273,\n    1727=1274,\n    107=1275,\n    1237=1276,\n    775=1277,\n    356=1278,\n    1832=1279,\n    660=1280,\n    1854=1281,\n    2400=1282,\n    1381=1283,\n    2436=1284,\n    901=1285,\n    19=1286,\n    808=1287,\n    2131=1288,\n    1382=1289,\n    406=1290,\n    357=1291,\n    348=1292,\n    407=1293,\n    1389=1294,\n    724=1295,\n    2313=1296,\n    496=1297,\n    1630=1298,\n    1810=1299,\n    756=1300,\n    887=1301,\n    977=1302,\n    921=1303,\n    922=1304,\n    720=1305,\n    851=1306,\n    721=1307,\n    1100=1308,\n    2144=1309,\n    1095=1310,\n    1953=1311,\n    1145=1312,\n    910=1313,\n    650=1314,\n    1870=1315,\n    2452=1316,\n    277=1317,\n    858=1318,\n    844=1319,\n    1908=1320,\n    845=1321,\n    2299=1322,\n    2488=1323,\n    2460=1324,\n    1710=1325,\n    2474=1326,\n    959=1327,\n    1815=1328,\n    1173=1329,\n    1141=1330,\n    1028=1331,\n    1454=1332,\n    1459=1333,\n    1831=1334,\n    827=1335,\n    2191=1336,\n    716=1337,\n    855=1338,\n    1909=1339,\n    461=1340,\n    999=1341,\n    2027=1342,\n    1484=1343,\n    2344=1344,\n    1103=1345,\n    1090=1346,\n    1654=1347,\n    2143=1348,\n    816=1349,\n    2042=1350,\n    325=1351,\n    1246=1352,\n    359=1353,\n    463=1354,\n    2028=1355,\n    1154=1356,\n    1071=1357,\n    223=1358,\n    840=1359,\n    929=1360,\n    917=1361,\n    221=1362,\n    14=1363,\n    335=1364,\n    91=1365,\n    2485=1366,\n    470=1367,\n    1683=1368,\n    1318=1369,\n    1782=1370,\n    1812=1371,\n    597=1372,\n    1711=1373,\n    849=1374,\n    1271=1375,\n    1303=1376,\n    2206=1377,\n    2066=1378,\n    1966=1379,\n    1912=1380,\n    245=1381,\n    474=1382,\n    469=1383,\n    1861=1384,\n    212=1385,\n    1867=1386,\n    1153=1387,\n    1439=1388,\n    149=1389,\n    607=1390,\n    1149=1391,\n    1785=1392,\n    1472=1393,\n    853=1394,\n    1726=1395,\n    1922=1396,\n    491=1397,\n    1247=1398,\n    708=1399,\n    2435=1400,\n    2352=1401,\n    1169=1402,\n    1992=1403,\n    1032=1404,\n    1133=1405,\n    647=1406,\n    6=1407,\n    199=1408,\n    2263=1409,\n    799=1410,\n    2420=1411,\n    2319=1412,\n    1974=1413,\n    632=1414,\n    2258=1415,\n    3=1416,\n    2347=1417,\n    2450=1418,\n    839=1419,\n    1304=1420,\n    1273=1421,\n    1025=1422,\n    186=1423,\n    1629=1424,\n    538=1425,\n    2470=1426,\n    405=1427,\n    1274=1428,\n    1986=1429,\n    1268=1430,\n    310=1431,\n    1457=1432,\n    638=1433,\n    762=1434,\n    1132=1435,\n    824=1436,\n    1012=1437,\n    974=1438,\n    1882=1439,\n    309=1440,\n    370=1441,\n    401=1442,\n    1510=1443,\n    66=1444,\n    444=1445,\n    1140=1446,\n    2054=1447,\n    54=1448,\n    55=1449,\n    2390=1450,\n    1708=1451,\n    1396=1452,\n    1349=1453,\n    64=1454,\n    641=1455,\n    652=1456,\n    50=1457,\n    447=1458,\n    1651=1459,\n    1939=1460,\n    742=1461,\n    195=1462,\n    26=1463,\n    404=1464,\n    930=1465,\n    2195=1466,\n    1749=1467,\n    1819=1468,\n    1736=1469,\n    1272=1470,\n    428=1471,\n    704=1472,\n    1853=1473,\n    655=1474,\n    608=1475,\n    1679=1476,\n    1523=1477,\n    395=1478,\n    1542=1479,\n    850=1480,\n    1941=1481,\n    1096=1482,\n    196=1483,\n    649=1484,\n    2407=1485,\n    759=1486,\n    18=1487,\n    2009=1488,\n    2087=1489,\n    614=1490,\n    1664=1491,\n    366=1492,\n    293=1493,\n    333=1494,\n    516=1495,\n    482=1496,\n    483=1497,\n    2236=1498,\n    736=1499,\n    739=1500,\n    1719=1501,\n    287=1502,\n    2290=1503,\n    1632=1504,\n    926=1505,\n    94=1506,\n    86=1507,\n    664=1508,\n    204=1509,\n    1856=1510,\n    70=1511,\n    1288=1512,\n    237=1513,\n    2286=1514,\n    1813=1515,\n    207=1516,\n    758=1517,\n    1830=1518,\n    873=1519,\n    1482=1520,\n    782=1521,\n    383=1522,\n    543=1523,\n    1723=1524,\n    301=1525,\n    157=1526,\n    2059=1527,\n    577=1528,\n    703=1529,\n    1257=1530,\n    1792=1531,\n    159=1532,\n    2500=1533,\n    2603=1534,\n    2744=1535,\n    2645=1536,\n    2788=1537,\n    2545=1538,\n    2705=1539,\n    2496=1540,\n    2755=1541,\n    2611=1542,\n    2489=1543,\n    2751=1544,\n    2499=1545,\n    2071=1546,\n    2103=1547,\n    1295=1548,\n    617=1549,\n    2178=1550,\n    1346=1551,\n    2229=1552,\n    1043=1553,\n    902=1554,\n    1741=1555,\n    153=1556,\n    2306=1557,\n    797=1558,\n    150=1559,\n    1945=1560,\n    2090=1561,\n    672=1562,\n    9=1563,\n    1997=1564,\n    1665=1565,\n    270=1566,\n    1026=1567,\n    1448=1568,\n    1097=1569,\n    1447=1570,\n    1995=1571,\n    1520=1572,\n    2121=1573,\n    1713=1574,\n    1681=1575,\n    1717=1576,\n    741=1577,\n    1845=1578,\n    1108=1579,\n    1597=1580,\n    829=1581,\n    987=1582,\n    511=1583,\n    731=1584,\n    82=1585,\n    2068=1586,\n    1194=1587,\n    78=1588,\n    397=1589,\n    450=1590,\n    2072=1591,\n    2113=1592,\n    2253=1593,\n    837=1594,\n    669=1595,\n    96=1596,\n    1760=1597,\n    1801=1598,\n    2283=1599,\n    2754=1600,\n    2745=1601,\n    1791=1602,\n    527=1603,\n    531=1604,\n    774=1605,\n    1394=1606,\n    25=1607,\n    2278=1608,\n    2361=1609,\n    2213=1610,\n    1347=1611,\n    1848=1612,\n    1887=1613,\n    1570=1614,\n    729=1615,\n    986=1616,\n    1701=1617,\n    818=1618,\n    2457=1619,\n    600=1620,\n    735=1621,\n    2017=1622,\n    562=1623,\n    138=1624,\n    564=1625,\n    1787=1626,\n    744=1627,\n    1892=1628,\n    1360=1629,\n    2444=1630,\n    1667=1631,\n    1067=1632,\n    1783=1633,\n    1627=1634,\n    612=1635,\n    1339=1636,\n    1264=1637,\n    1319=1638,\n    2221=1639,\n    2232=1640,\n    1752=1641,\n    723=1642,\n    1543=1643,\n    350=1644,\n    1119=1645,\n    755=1646,\n    248=1647,\n    1590=1648,\n    2125=1649,\n    1138=1650,\n    63=1651,\n    2412=1652,\n    1698=1653,\n    1127=1654,\n    654=1655,\n    2358=1656,\n    871=1657,\n    841=1658,\n    1428=1659,\n    1946=1660,\n    1900=1661,\n    1907=1662,\n    116=1663,\n    1131=1664,\n    909=1665,\n    2250=1666,\n    1834=1667,\n    2312=1668,\n    794=1669,\n    2070=1670,\n    1973=1671,\n    1776=1672,\n    1775=1673,\n    1687=1674,\n    795=1675,\n    165=1676,\n    1393=1677,\n    1156=1678,\n    8=1679,\n    949=1680,\n    1680=1681,\n    1905=1682,\n    2150=1683,\n    1236=1684,\n    1703=1685,\n    2078=1686,\n    616=1687,\n    2387=1688,\n    1327=1689,\n    313=1690,\n    443=1691,\n    49=1692,\n    1581=1693,\n    1152=1694,\n    1797=1695,\n    1506=1696,\n    1372=1697,\n    2098=1698,\n    2419=1699,\n    2315=1700,\n    2053=1701,\n    599=1702,\n    1477=1703,\n    2482=1704,\n    1409=1705,\n    685=1706,\n    1079=1707,\n    2481=1708,\n    488=1709,\n    2456=1710,\n    1369=1711,\n    1370=1712,\n    2208=1713,\n    1673=1714,\n    1948=1715,\n    862=1716,\n    1007=1717,\n    923=1718,\n    132=1719,\n    2389=1720,\n    2077=1721,\n    1566=1722,\n    2189=1723,\n    574=1724,\n    1392=1725,\n    2169=1726,\n    1750=1727,\n    121=1728,\n    1233=1729,\n    1872=1730,\n    1862=1731,\n    334=1732,\n    2252=1733,\n    2005=1734,\n    918=1735,\n    2455=1736,\n    628=1737,\n    2454=1738,\n    805=1739,\n    639=1740,\n    114=1741,\n    2247=1742,\n    1940=1743,\n    1768=1744,\n    510=1745,\n    1677=1746,\n    592=1747,\n    1240=1748,\n    2363=1749,\n    10=1750,\n    2043=1751,\n    42=1752,\n    187=1753,\n    1453=1754,\n    586=1755,\n    1023=1756,\n    307=1757,\n    22=1758,\n    1139=1759,\n    1742=1760,\n    522=1761,\n    2356=1762,\n    2355=1763,\n    95=1764,\n    2257=1765,\n    1460=1766,\n    878=1767,\n    965=1768,\n    1774=1769,\n    2461=1770,\n    596=1771,\n    585=1772,\n    60=1773,\n    1894=1774,\n    1333=1775,\n    1666=1776,\n    2244=1777,\n    1209=1778,\n    942=1779,\n    646=1780,\n    659=1781,\n    532=1782,\n    1740=1783,\n    1424=1784,\n    1636=1785,\n    1754=1786,\n    784=1787,\n    701=1788,\n    1860=1789,\n    113=1790,\n    1299=1791,\n    546=1792,\n    1302=1793,\n    184=1794,\n    2186=1795,\n    7=1796,\n    513=1797,\n    1298=1798,\n    1869=1799,\n    863=1800,\n    566=1801,\n    2300=1802,\n    905=1803,\n    1296=1804,\n    2211=1805,\n    2297=1806,\n    1124=1807,\n    970=1808,\n    969=1809,\n    1645=1810,\n    537=1811,\n    2376=1812,\n    2408=1813,\n    1716=1814,\n    2298=1815,\n    1967=1816,\n    1989=1817,\n    1502=1818,\n    497=1819,\n    1751=1820,\n    613=1821,\n    536=1822,\n    2055=1823,\n    1588=1824,\n    1535=1825,\n    550=1826,\n    365=1827,\n    152=1828,\n    1297=1829,\n    1035=1830,\n    2370=1831,\n    2231=1832,\n    1315=1833,\n    388=1834,\n    1376=1835,\n    1607=1836,\n    2721=1837,\n    2174=1838,\n    2636=1839,\n    2795=1840,\n    2590=1841,\n    1374=1842,\n    2648=1843,\n    2725=1844,\n    2642=1845,\n    2667=1846,\n    2800=1847,\n    2752=1848,\n    2739=1849,\n    2696=1850,\n    2567=1851,\n    2568=1852,\n    2761=1853,\n    2762=1854,\n    1375=1855,\n    2549=1856,\n    2184=1857,\n    2584=1858,\n    2525=1859,\n    2573=1860,\n    2576=1861,\n    2536=1862,\n    2807=1863,\n    2574=1864,\n    2582=1865,\n    2580=1866,\n    2581=1867,\n    2575=1868,\n    2759=1869,\n    2569=1870,\n    2663=1871,\n    2532=1872,\n    2672=1873,\n    2664=1874,\n    2615=1875,\n    2785=1876,\n    2490=1877,\n    2662=1878,\n    2758=1879,\n    2635=1880,\n    1069=1881,\n    2673=1882,\n    2554=1883,\n    2671=1884,\n    2770=1885,\n    2516=1886,\n    2709=1887,\n    2614=1888,\n    2535=1889,\n    2637=1890,\n    2661=1891,\n    2674=1892,\n    2774=1893,\n    954=1894,\n    2715=1895,\n    2686=1896,\n    2766=1897,\n    2802=1898,\n    2646=1899,\n    2806=1900,\n    2726=1901,\n    2714=1902,\n    2505=1903,\n    2650=1904,\n    2803=1905,\n    2668=1906,\n    2805=1907,\n    2530=1908,\n    2509=1909,\n    2654=1910,\n    2553=1911,\n    2809=1912,\n    2773=1913,\n    2011=1914,\n    1762=1915,\n    2127=1916,\n    2233=1917,\n    87=1918,\n    897=1919,\n    1509=1920,\n    1572=1921,\n    45=1922,\n    1634=1923,\n    412=1924,\n    1579=1925,\n    2056=1926,\n    1177=1927,\n    996=1928,\n    579=1929,\n    2260=1930,\n    1042=1931,\n    472=1932,\n    1081=1933,\n    2219=1934,\n    494=1935,\n    1884=1936,\n    299=1937,\n    261=1938,\n    780=1939,\n    1772=1940,\n    2289=1941,\n    1193=1942,\n    699=1943,\n    943=1944,\n    231=1945,\n    128=1946,\n    260=1947,\n    2136=1948,\n    1513=1949,\n    1638=1950,\n    1361=1951,\n    1549=1952,\n    2402=1953,\n    1804=1954,\n    2334=1955,\n    2275=1956,\n    2046=1957,\n    2651=1958,\n    2406=1959,\n    2163=1960,\n    2166=1961,\n    768=1962,\n    2421=1963,\n    1114=1964,\n    2080=1965,\n    1612=1966,\n    2385=1967,\n    1184=1968,\n    268=1969,\n    737=1970,\n    539=1971,\n    2106=1972,\n    880=1973,\n    1442=1974,\n    2264=1975,\n    1312=1976,\n    1921=1977,\n    2037=1978,\n    2039=1979,\n    2181=1980,\n    2365=1981,\n    1969=1982,\n    1571=1983,\n    953=1984,\n    1618=1985,\n    1982=1986,\n    2618=1987,\n    1546=1988,\n    1777=1989,\n    235=1990,\n    687=1991,\n    1417=1992,\n    2155=1993,\n    1283=1994,\n    2202=1995,\n    305=1996,\n    501=1997,\n    290=1998,\n    419=1999,\n    58=2000,\n    1170=2001,\n    1162=2002,\n    500=2003,\n    1626=2004,\n    2797=2005,\n    2432=2006,\n    122=2007,\n    1859=2008,\n    787=2009,\n    2246=2010,\n    1583=2011,\n    1045=2012,\n    1255=2013,\n    492=2014,\n    2453=2015,\n    2285=2016,\n    1038=2017,\n    1395=2018,\n    440=2019,\n    1087=2020,\n    429=2021,\n    830=2022,\n    33=2023,\n    857=2024,\n    1405=2025,\n    1821=2026,\n    1496=2027,\n    990=2028,\n    1690=2029,\n    417=2030,\n    1988=2031,\n    1486=2032,\n    467=2033,\n    319=2034,\n    966=2035,\n    439=2036,\n    1594=2037,\n    1593=2038,\n    1928=2039,\n    1426=2040,\n    1961=2041,\n    1265=2042,\n    570=2043,\n    1532=2044,\n    1878=2045,\n    61=2046,\n    750=2047,\n    706=2048,\n    133=2049,\n    495=2050,\n    1575=2051,\n    1876=2052,\n    1576=2053,\n    1826=2054,\n    1901=2055,\n    1820=2056,\n    1824=2057,\n    236=2058,\n    1616=2059,\n    2371=2060,\n    1623=2061,\n    1202=2062,\n    1422=2063,\n    1065=2064,\n    1559=2065,\n    1558=2066,\n    198=2067,\n    2142=2068,\n    747=2069,\n    2337=2070,\n    1955=2071,\n    2414=2072,\n    170=2073,\n    250=2074,\n    1321=2075,\n    572=2076,\n    2203=2077,\n    1689=2078,\n    1614=2079,\n    1624=2080,\n    2013=2081,\n    1061=2082,\n    2082=2083,\n    1818=2084,\n    1397=2085,\n    751=2086,\n    44=2087,\n    2327=2088,\n    2310=2089,\n    1197=2090,\n    1548=2091,\n    252=2092,\n    1063=2093,\n    2570=2094,\n    2074=2095,\n    453=2096,\n    2557=2097,\n    2665=2098,\n    2753=2099,\n    2792=2100,\n    2794=2101,\n    2798=2102,\n    2551=2103,\n    2099=2104,\n    2555=2105,\n    1406=2106,\n    2523=2107,\n    541=2108,\n    1253=2109,\n    869=2110,\n    801=2111,\n    1518=2112,\n    1497=2113,\n    1210=2114,\n    1598=2115,\n    1285=2116,\n    1029=2117,\n    2544=2118,\n    1116=2119,\n    793=2120,\n    2000=2121,\n    2609=2122,\n    2561=2123,\n    2638=2124,\n    1008=2125,\n    2547=2126,\n    2188=2127,\n    2589=2128,\n    2628=2129,\n    2741=2130,\n    2742=2131,\n    2698=2132,\n    2675=2133,\n    2624=2134,\n    2644=2135,\n    1675=2136,\n    2613=2137,\n    811=2138,\n    785=2139,\n    1693=2140,\n    2808=2141,\n    2643=2142,\n    2533=2143,\n    2518=2144,\n    2690=2145,\n    2692=2146,\n    2687=2147,\n    2095=2148,\n    2813=2149,\n    2631=2150,\n    2578=2151,\n    2804=2152,\n    2031=2153,\n    1596=2154,\n    140=2155,\n    912=2156,\n    1161=2157,\n    1960=2158,\n    2239=2159,\n    139=2160,\n    1064=2161,\n    1962=2162,\n    1695=2163,\n    2002=2164,\n    961=2165,\n    693=2166,\n    2413=2167,\n    670=2168,\n    1205=2169,\n    1758=2170,\n    1410=2171,\n    2556=2172,\n    2729=2173,\n    2649=2174,\n    2510=2175,\n    2504=2176,\n    2670=2177,\n    2565=2178,\n    2605=2179,\n    662=2180,\n    984=2181,\n    682=2182,\n    1475=2183,\n    1039=2184,\n    2473=2185,\n    175=2186,\n    817=2187,\n    1530=2188,\n    2360=2189,\n    2004=2190,\n    2019=2191,\n    422=2192,\n    950=2193,\n    879=2194,\n    593=2195,\n    2049=2196,\n    1289=2197,\n    2324=2198,\n    2048=2199,\n    403=2200,\n    2085=2201,\n    770=2202,\n    197=2203,\n    2392=2204,\n    2345=2205,\n    1175=2206,\n    1839=2207,\n    1931=2208,\n    2587=2209,\n    2606=2210,\n    2607=2211,\n    2562=2212,\n    2727=2213,\n    2779=2214,\n    2647=2215,\n    2746=2216,\n    719=2217,\n    2685=2218,\n    2750=2219,\n    468=2220,\n    2475=2221,\n    262=2222,\n    1362=2223,\n    952=2224,\n    914=2225,\n    1526=2226,\n    1109=2227,\n    1688=2228,\n    15=2229,\n    2317=2230,\n    225=2231,\n    20=2232,\n    1527=2233,\n    1910=2234,\n    457=2235,\n    1811=2236,\n    2763=2237,\n    2699=2238,\n    2632=2239,\n    2700=2240,\n    2771=2241,\n    2777=2242,\n    1052=2243,\n    2084=2244,\n    146=2245,\n    2197=2246,\n    2483=2247,\n    2041=2248,\n    1427=2249,\n    2790=2250,\n    65=2251,\n    2428=2252,\n    2138=2253,\n    1001=2254,\n    2633=2255,\n    2538=2256,\n    2029=2257,\n    964=2258,\n    2511=2259,\n    2598=2260,\n    2634=2261,\n    1553=2262,\n    1387=2263,\n    2617=2264,\n    2540=2265,\n    2799=2266,\n    2738=2267,\n    2601=2268,\n    1803=2269,\n    1808=2270,\n    1403=2271,\n    84=2272,\n    289=2273,\n    330=2274,\n    890=2275,\n    2223=2276,\n    451=2277,\n    1891=2278,\n    2443=2279,\n    726=2280,\n    1367=2281,\n    436=2282,\n    1158=2283,\n    1503=2284,\n    1160=2285,\n    2772=2286,\n    2693=2287,\n    1359=2288,\n    1037=2289,\n    279=2290,\n    796=2291,\n    753=2292,\n    1072=2293,\n    445=2294,\n    2292=2295,\n    59=2296,\n    1952=2297,\n    234=2298,\n    2564=2299,\n    2237=2300,\n    1574=2301,\n    951=2302,\n    1166=2303,\n    2057=2304,\n    524=2305,\n    1611=2306,\n    2063=2307,\n    2384=2308,\n    1789=2309,\n    958=2310,\n    1949=2311,\n    738=2312,\n    540=2313,\n    2107=2314,\n    881=2315,\n    2265=2316,\n    2040=2317,\n    2180=2318,\n    2364=2319,\n    1968=2320,\n    1591=2321,\n    955=2322,\n    601=2323,\n    1311=2324,\n    1539=2325,\n    173=2326,\n    928=2327,\n    1364=2328,\n    1404=2329,\n    1896=2330,\n    143=2331,\n    2349=2332,\n    1577=2333,\n    118=2334,\n    1353=2335,\n    882=2336,\n    1495=2337,\n    327=2338,\n    1560=2339,\n    1089=2340,\n    1564=2341,\n    2514=2342,\n    864=2343,\n    1228=2344,\n    1798=2345,\n    1182=2346,\n    1643=2347,\n    481=2348,\n    765=2349,\n    677=2350,\n    813=2351,\n    2449=2352,\n    1293=2353,\n    848=2354,\n    399=2355,\n    779=2356,\n    1516=2357,\n    1167=2358,\n    1373=2359,\n    322=2360,\n    2279=2361,\n    2179=2362,\n    129=2363,\n    181=2364,\n    1111=2365,\n    1938=2366,\n    967=2367,\n    1322=2368,\n    764=2369,\n    135=2370,\n    1443=2371,\n    251=2372,\n    62=2373,\n    1879=2374,\n    2429=2375,\n    75=2376,\n    1117=2377,\n    2001=2378,\n    2190=2379,\n    1585=2380,\n    2036=2381,\n    254=2382,\n    35=2383,\n    1927=2384,\n    2117=2385,\n    519=2386,\n    728=2387,\n    2354=2388,\n    807=2389,\n    1557=2390,\n    56=2391,\n    571=2392,\n    1531=2393,\n    769=2394,\n    1441=2395,\n    2422=2396,\n    464=2397,\n    559=2398,\n    1325=2399,\n    563=2400,\n    587=2401,\n    1267=2402,\n    2403=2403,\n    781=2404,\n    97=2405,\n    272=2406,\n    1764=2407,\n    883=2408,\n    2065=2409,\n    2478=2410,\n    216=2411,\n    1134=2412,\n    1981=2413,\n    754=2414,\n    2102=2415,\n    2100=2416,\n    1844=2417,\n    2472=2418,\n    1920=2419,\n    266=2420,\n    1970=2421,\n    1587=2422,\n    1031=2423,\n    1983=2424,\n    916=2425,\n    1846=2426,\n    573=2427,\n    1669=2428,\n    1355=2429,\n    2094=2430,\n    2139=2431,\n    2224=2432,\n    1435=2433,\n    1022=2434,\n    1582=2435,\n    119=2436,\n    1354=2437,\n    98=2438,\n    2366=2439,\n    1452=2440,\n    1712=2441,\n    1748=2442,\n    343=2443,\n    324=2444,\n    2101=2445,\n    281=2446,\n    2170=2447,\n    2368=2448,\n    1244=2449,\n    34=2450,\n    1399=2451,\n    1613=2452,\n    418=2453,\n    2434=2454,\n    1663=2455,\n    2182=2456,\n    2091=2457,\n    718=2458,\n    1088=2459,\n    1682=2460,\n    100=2461,\n    1584=2462,\n    2021=2463,\n    552=2464,\n    2255=2465,\n    361=2466,\n    1017=2467,\n    1270=2468,\n    414=2469,\n    2081=2470,\n    1198=2471,\n    1732=2472,\n    2133=2473,\n    332=2474,\n    1366=2475,\n    315=2476,\n    1129=2477,\n    2810=2478,\n    2016=2479,\n    946=2480,\n    1715=2481,\n    106=2482,\n    763=2483,\n    2323=2484,\n    1402=2485,\n    1432=2486,\n    459=2487,\n    569=2488,\n    788=2489,\n    1282=2490,\n    1494=2491,\n    1841=2492,\n    1207=2493,\n    1101=2494,\n    1550=2495,\n    1640=2496,\n    2271=2497,\n    166=2498,\n    249=2499,\n    621=2500,\n    452=2501,\n    710=2502,\n    142=2503,\n    2626=2504,\n    2711=2505,\n    2158=2506,\n    1430=2507,\n    1425=2508,\n    2541=2509,\n    1341=2510,\n    870=2511,\n    2507=2512,\n    809=2513,\n    2369=2514,\n    1733=2515,\n    2156=2516,\n    1499=2517,\n    1080=2518,\n    2287=2519,\n    1338=2520,\n    1400=2521,\n    81=2522,\n    1753=2523,\n    1555=2524,\n    1476=2525,\n    1807=2526,\n    167=2527,\n    535=2528,\n    696=2529,\n    380=2530,\n    1842=2531,\n    1672=2532,\n    39=2533,\n    1368=2534,\n    994=2535,\n    992=2536,\n    437=2537,\n    490=2538,\n    329=2539,\n    629=2540,\n    168=2541,\n    1504=2542,\n    2256=2543,\n    2261=2544,\n    182=2545,\n    1802=2546,\n    448=2547,\n    1533=2548,\n    2276=2549,\n    438=2550,\n    2332=2551,\n    2280=2552,\n    991=2553,\n    2062=2554,\n    340=2555,\n    2171=2556,\n    2238=2557,\n    1059=2558,\n    1794=2559,\n    1692=2560,\n    1105=2561,\n    326=2562,\n    1898=2563,\n    1505=2564,\n    709=2565,\n    1421=2566,\n    1621=2567,\n    1099=2568,\n    124=2569,\n    1987=2570,\n    2519=2571,\n    16=2572,\n    2620=2573,\n    2600=2574,\n    1642=2575,\n    553=2576,\n    336=2577,\n    1221=2578,\n    1895=2579,\n    1511=2580,\n    372=2581,\n    76=2582,\n    825=2583,\n    2134=2584,\n    88=2585,\n    2307=2586,\n    505=2587,\n    1700=2588,\n    282=2589,\n    567=2590,\n    625=2591,\n    1852=2592,\n    2571=2593,\n    2710=2594,\n    2501=2595,\n    367=2596,\n    746=2597,\n    634=2598,\n    525=2599,\n    1151=2600,\n    1262=2601,\n    2393=2602,\n    376=2603,\n    1881=2604,\n    661=2605,\n    338=2606,\n    102=2607,\n    831=2608,\n    442=2609,\n    425=2610,\n    1840=2611,\n    2503=2612,\n    51=2613,\n    24=2614,\n    1308=2615,\n    2702=2616,\n    2616=2617,\n    2521=2618,\n    2513=2619,\n    2708=2620,\n    2308=2621,\n    1933=2622,\n    1230=2623,\n    1936=2624,\n    1078=2625,\n    1745=2626,\n    1685=2627,\n    2336=2628,\n    2050=2629,\n    2520=2630,\n    1902=2631,\n    462=2632,\n    2563=2633,\n    275=2634,\n    1050=2635,\n    972=2636,\n    868=2637,\n    17=2638,\n    2230=2639,\n    973=2640,\n    227=2641,\n    2214=2642,\n    688=2643,\n    2703=2644,\n    1538=2645,\n    1947=2646,\n    588=2647,\n    2586=2648,\n    27=2649,\n    2351=2650,\n    1434=2651,\n    715=2652,\n    2512=2653,\n    2688=2654,\n    1637=2655,\n    1490=2656,\n    1492=2657,\n    1493=2658,\n    136=2659,\n    935=2660,\n    1705=2661,\n    1707=2662,\n    1185=2663,\n    900=2664,\n    1168=2665,\n    1323=2666,\n    1412=2667,\n    1044=2668,\n    1203=2669,\n    1877=2670,\n    2064=2671,\n    344=2672,\n    2534=2673,\n    2689=2674,\n    144=2675,\n    1345=2676,\n    653=2677,\n    2425=2678,\n    423=2679,\n    663=2680,\n    713=2681,\n    2248=2682,\n    1458=2683,\n    1814=2684,\n    1013=2685,\n    2566=2686,\n    2666=2687,\n    2640=2688,\n    2639=2689,\n    2592=2690,\n    2737=2691,\n    2791=2692,\n    2604=2693,\n    200=2694,\n    285=2695,\n    534=2696,\n    1823=2697,\n    394=2698,\n    2733=2699,\n    2492=2700,\n    1094=2701,\n    2165=2702,\n    2720=2703,\n    2691=2704,\n    2550=2705,\n    316=2706,\n    2395=2707,\n    267=2708,\n    411=2709,\n    2506=2710,\n    2585=2711,\n    2491=2712,\n    2526=2713,\n    2768=2714,\n    2679=2715,\n    2657=2716,\n    2682=2717,\n    2684=2718\n  },\n  positive_pairs=[6169, 2],\n  negative_pairs=[1000, 2],\n  val_pos_edge_index=[2, 308],\n  test_pos_edge_index=[2, 616],\n  train_pos_edge_index=[2, 10490],\n  train_neg_adj_mask=[2719, 2719],\n  val_neg_edge_index=[2, 308],\n  test_neg_edge_index=[2, 616],\n  edge_index=[2, 12338]\n)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_edge_index = data.edge_index\n",
    "data = train_test_split_edges(data)\n",
    "data.edge_index = data_edge_index\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:56:03.974143Z",
     "end_time": "2023-04-11T10:56:04.080249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a link predictor model\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels*2, 1)\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "link_predictor = LinkPredictor(gae_model.encoder.out_channels)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(link_predictor.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 201):\n",
    "    optimizer.zero_grad()\n",
    "    out = gae_model.encoder(data.x, data.edge_index)\n",
    "    edge_scores = link_predictor(out[data.train_pos_edge_index[0]], out[data.train_pos_edge_index[1]])\n",
    "    ones = torch.ones(edge_scores.size(0), dtype=torch.float)\n",
    "    ones = ones.unsqueeze(1)\n",
    "    loss = criterion(edge_scores, ones)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        accs = []\n",
    "        # for prefix in [\"train\", \"val\", \"test\"]:\n",
    "        pos_edge_index = data.train_pos_edge_index.t()\n",
    "        neg_edge_index = data.train_neg_adj_mask.t()\n",
    "        all_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=0)\n",
    "        out = gae_model.encoder(data.x, data.edge_index)\n",
    "        pos_edge_scores = link_predictor(out[pos_edge_index[0]], out[pos_edge_index[1]])\n",
    "        neg_edge_scores = link_predictor(out[neg_edge_index[0]], out[neg_edge_index[1]])\n",
    "        edge_scores = torch.cat([pos_edge_scores, neg_edge_scores])\n",
    "        pred = (edge_scores > 0).long()\n",
    "        label = torch.cat([torch.ones(pos_edge_index.size(1)), torch.zeros(neg_edge_index.size(1))]).long()\n",
    "        acc = (pred == label).float().mean()\n",
    "        accs.append(acc.item())\n",
    "        print(f\"Epoch {epoch:03d}\", f\"Train: {accs[0]:.4f}\", f\"Val: {accs[1]:.4f}\", f\"Test: {accs[2]:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T10:46:03.137586Z",
     "end_time": "2023-04-11T10:46:08.104443Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
